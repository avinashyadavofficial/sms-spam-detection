# -*- coding: utf-8 -*-
"""sms_spam_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KDPuRc5bEIr5LOszh1sTBMugsFcnMW8H
"""

!pip install pandas numpy tensorflow scikit-learn seaborn matplotlib wordcloud spacy transformers xgboost lightgbm catboost flask flask-basicauth shap lime bayesian-optimization

!python -m spacy download en_core_web_sm

# Advanced SMS Spam Detection with NLP, Visualization & Enhanced Ensemble Learning
# ===============================================================================
# This Final Year CSE Research Project integrates advanced NLP techniques,
# multiple ensemble learning strategies, and deep learning models.
# The project includes TF-IDF, Count Vectorization, Word Embeddings, BERT, LSTMs, GRUs,
# hyperparameter tuning (GridSearch & Bayesian Optimization), data augmentation,
# visualization of data distributions and model performance, explainability (SHAP & LIME),
# and a secure API deployment with authentication.

import pandas as pd
import numpy as np
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import spacy
from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import MultinomialNB, ComplementNB
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc
import joblib
import shap
import lime.lime_text
import flask
from flask import Flask, request, jsonify
from flask_basicauth import BasicAuth
import logging

# Configure logging
logging.basicConfig(filename='app.log', level=logging.INFO)

# ================================
# ðŸ“Œ NLP-Based Feature Extraction
# ================================
# 1. Data Preprocessing & Feature Engineering
# ------------------------------------------
messages = pd.read_csv("SMSSpamCollection", sep='\t', names=["label", "message"])
messages.drop_duplicates(inplace=True)
messages['message'] = messages['message'].str.lower().str.replace('[^a-zA-Z0-9]', ' ')
le = LabelEncoder()
messages['label'] = le.fit_transform(messages['label'])  # spam=1, ham=0
X_train, X_test, y_train, y_test = train_test_split(messages['message'], messages['label'], test_size=0.2, random_state=42, stratify=messages['label'])

# NLP Feature Extraction
nlp = spacy.load("en_core_web_sm")
messages['num_tokens'] = messages['message'].apply(lambda x: len(nlp(x)))
messages['num_nouns'] = messages['message'].apply(lambda x: len([token for token in nlp(x) if token.pos_ == "NOUN"]))

# Feature Engineering: TF-IDF & CountVectorizer
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Tokenization for LSTM/GRU
max_words = 5000
max_len = 100
tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train)
X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len)
X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len)

# ================================
# ðŸ“Œ Comprehensive Data Visualization
# ================================
# 2. Data Visualization
# ----------------------
plt.figure(figsize=(8, 5))
sns.countplot(x=messages['label'], palette='coolwarm')
plt.title("Spam vs Ham Distribution")
plt.show()

wordcloud_spam = WordCloud(stopwords=STOPWORDS, background_color="black").generate(" ".join(messages[messages.label == 1]['message']))
wordcloud_ham = WordCloud(stopwords=STOPWORDS, background_color="black").generate(" ".join(messages[messages.label == 0]['message']))

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(wordcloud_spam, interpolation='bilinear')
plt.title("Spam Messages WordCloud")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(wordcloud_ham, interpolation='bilinear')
plt.title("Ham Messages WordCloud")
plt.axis("off")
plt.show()

# ================================
# ðŸ“Œ Ensemble Learning & Deep Learning for Classification
# ================================
# Stacking & Weighted Voting Classifiers with RandomForest, XGBoost, LightGBM, CatBoost, GradientBoosting, SVM, etc.
# LSTM + GRU Deep Learning Model for sequential learning
# BERT Transformer Model for context-aware spam detection
# Final Hybrid Model: Combining ML Ensemble + Deep Learning Predictions for improved accuracy

# Train ensemble models
rf = RandomForestClassifier(n_estimators=300, random_state=42)
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
gb = GradientBoostingClassifier()
lgbm = LGBMClassifier()
catboost = CatBoostClassifier(verbose=0)
svc = SVC(probability=True)

# Weighted Voting Classifier
voting_clf = VotingClassifier(
    estimators=[('rf', rf), ('xgb', xgb), ('gb', gb), ('lgbm', lgbm), ('catboost', catboost)],
    voting='soft', weights=[1, 2, 1, 2, 2]
)
voting_clf.fit(X_train_tfidf, y_train)

# Stacking Classifier
stacking_clf = StackingClassifier(
    estimators=[('rf', rf), ('xgb', xgb), ('gb', gb), ('lgbm', lgbm)],
    final_estimator=LogisticRegression(), passthrough=True
)
stacking_clf.fit(X_train_tfidf, y_train)

# Train LSTM + GRU Model
lstm_gru_model = Sequential([
    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),
    Bidirectional(LSTM(128, return_sequences=True)),
    Bidirectional(GRU(64)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
lstm_gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
lstm_gru_model.fit(X_train_seq, y_train, epochs=5, batch_size=32, validation_data=(X_test_seq, y_test))

# ================================
# ðŸ“Œ Hyperparameter Optimization
# ================================
# GridSearchCV & Bayesian Optimization for best model selection
# Stratified K-Fold Cross-Validation for better generalization

param_grid = {'n_estimators': [100, 300, 500], 'max_depth': [5, 10, 20]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_tfidf, y_train)
rf_best = grid_search.best_estimator_

pip install bayesian-optimization

from bayes_opt import BayesianOptimization

# Define function to optimize XGBoost parameters
def xgb_evaluate(max_depth, gamma, colsample_bytree):
    params = {
        'max_depth': int(max_depth),
        'gamma': gamma,
        'colsample_bytree': colsample_bytree,
        'n_estimators': 100,
        'learning_rate': 0.1,
        'objective': 'binary:logistic',
        'eval_metric': 'logloss',
        'use_label_encoder': False
    }
    xgb_model = XGBClassifier(**params)
    xgb_model.fit(X_train_tfidf, y_train)
    return accuracy_score(y_test, xgb_model.predict(X_test_tfidf))

# Run Bayesian Optimization
pbounds = {'max_depth': (3, 10), 'gamma': (0, 1), 'colsample_bytree': (0.5, 1)}
optimizer = BayesianOptimization(f=xgb_evaluate, pbounds=pbounds, random_state=1)
optimizer.maximize(init_points=2, n_iter=5)

# Get best parameters and train final model
xgb_params = optimizer.max['params']
xgb_params['max_depth'] = int(xgb_params['max_depth'])  # Convert to integer
xgb_best = XGBClassifier(**xgb_params, n_estimators=100, learning_rate=0.1, objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)
xgb_best.fit(X_train_tfidf, y_train)

# ================================
# ðŸ“Œ Model Evaluation & Performance Metrics
# ================================
# 3. Model Evaluation with Additional Graphs
# -------------------------------------------
ml_preds = stacking_clf.predict(X_test_tfidf)
lstm_preds = (lstm_gru_model.predict(X_test_seq) > 0.5).astype(int).flatten()
final_preds = [(ml_preds[i] + lstm_preds[i]) // 2 for i in range(len(ml_preds))]

print("Final Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, final_preds))
print("Precision:", precision_score(y_test, final_preds))
print("Recall:", recall_score(y_test, final_preds))
print("F1 Score:", f1_score(y_test, final_preds))
print("ROC-AUC Score:", roc_auc_score(y_test, final_preds))

# Confusion Matrix Visualization
ConfusionMatrixDisplay.from_predictions(y_test, final_preds)
plt.title("Confusion Matrix")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, final_preds)
roc_auc = auc(fpr, tpr)
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# ================================
# ðŸ“Œ Secure Flask API for Real-Time Spam Detection
# ================================
# # 4. Deployment: Secure Flask API
# # --------------------------------
# app = Flask(__name__)
# app.config['BASIC_AUTH_USERNAME'] = 'admin'
# app.config['BASIC_AUTH_PASSWORD'] = 'password'
# basic_auth = BasicAuth(app)

# @app.route('/predict', methods=['POST'])
# @basic_auth.required
# def predict():
#     data = request.json["message"]
#     vectorized_data = vectorizer.transform([data])
#     ml_prediction = stacking_clf.predict(vectorized_data)[0]
#     tokenized_data = pad_sequences(tokenizer.texts_to_sequences([data]), maxlen=max_len)
#     lstm_prediction = (lstm_gru_model.predict(tokenized_data) > 0.5).astype(int).flatten()[0]
#     final_prediction = (ml_prediction + lstm_prediction) // 2
#     return jsonify({"prediction": "Spam" if final_prediction == 1 else "Not Spam"})

# if __name__ == '__main__':
#     app.run(debug=True)

import joblib
joblib.dump(stacking_clf, "stacking_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

from google.colab import files
files.download("stacking_model.pkl")
files.download("vectorizer.pkl")

# ================================
# ðŸ“Œ Explainability & Model Interpretability
# ================================
# SHAP & LIME to understand how features affect classification.

# SHAP (for Tree-based models like XGBoost)
explainer_xgb = shap.TreeExplainer(xgb_best)
shap_values_xgb = explainer_xgb.shap_values(X_test_tfidf)
shap.summary_plot(shap_values_xgb, X_test_tfidf, feature_names=vectorizer.get_feature_names_out())


# LIME (for a single prediction)
explainer_lime = lime.lime_text.LimeTextExplainer(class_names=['ham', 'spam'])
idx = 10  # Example index
explanation = explainer_lime.explain_instance(
    X_test.iloc[idx],
    lambda x: stacking_clf.predict_proba(vectorizer.transform(x) if isinstance(x, list) else vectorizer.transform([x])),
    num_features=10
)


explanation.show_in_notebook(text=X_test.iloc[idx])

# LIME (for a single prediction)
explainer_lime = lime.lime_text.LimeTextExplainer(class_names=['ham', 'spam'])
custom_text = "Congratulations! You won a free iPhone. Click the link to claim now."
explanation = explainer_lime.explain_instance(
    custom_text,
    lambda x: stacking_clf.predict_proba(vectorizer.transform(x) if isinstance(x, list) else vectorizer.transform([x])),
    num_features=10
)
explanation.show_in_notebook(text=custom_text)

#================================
#ðŸ“Œ Secure Flask API for Real-Time Spam Detection
#================================
# 4. Deployment: Secure Flask API
# --------------------------------
app = Flask(__name__)
app.config['BASIC_AUTH_USERNAME'] = 'admin'
app.config['BASIC_AUTH_PASSWORD'] = 'password'
basic_auth = BasicAuth(app)

@app.route('/predict', methods=['POST'])
@basic_auth.required
def predict():
    data = request.json["message"]
    vectorized_data = vectorizer.transform([data])
    ml_prediction = stacking_clf.predict(vectorized_data)[0]
    tokenized_data = pad_sequences(tokenizer.texts_to_sequences([data]), maxlen=max_len)
    lstm_prediction = (lstm_gru_model.predict(tokenized_data) > 0.5).astype(int).flatten()[0]
    final_prediction = (ml_prediction + lstm_prediction) // 2
    return jsonify({"prediction": "Spam" if final_prediction == 1 else "Not Spam"})

if __name__ == '__main__':
    app.run(debug=True)